{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8ca80e5-5cf2-48fa-a048-b10764f133c3"
      },
      "source": [
        "# Universidade de Brasília\n",
        "## Instituto de Física\n",
        "---\n",
        "### Métodos Computacionais A (MCA) \n",
        "#### Prof. Bernhard Enders\n",
        "---"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "060dc954-dbfb-4c68-99b7-35fa98d15b76"
      },
      "source": [
        "# **➲ AULA 13.12: Diferenciação Numérica**\n",
        "\n",
        "## Aproximações por Diferenças Finitas\n",
        "---\n",
        "\n",
        "A solução de uma equação diferencial (ED) implica na determinação dos valores da variável dependente em cada ponto do intervalo de interesse. Computacionalmente, podemos lidar apenas com uma região contínua se for possível determinar uma expressão analítica para a solução da equação diferencial. Nesse caso, o computador pode ser utilizado para calcular a solução em qualquer ponto desejado da região, com o uso da solução analítica. Contudo, no caso de técnicas numéricas de solução, não é possível tratar a região de interesse como contínua, já que, em geral, os métodos numéricos obtêm a solução do problema em pontos preestabelecidos. O processo de transformação do domínio contínuo em domínio discreto é chamado de discretização, onde o conjunto de pontos discretos escolhidos para representar o domínio de interesse é chamado de malha. A Figura abaixo mostra uma malha de pontos uniformemente espaçados, o passo da malha é definido como sendo a distância entre dois pontos adjacentes e é dado por $Δx = x_i − x_{i-1}$.\n",
        "\n",
        "<div style=\"margin:auto; text-align:center; width:75%;\">\n",
        "  <center>\n",
        "  <img src=\"https://raw.githubusercontent.com/bgeneto/MCA/main/imagens/malha.png\" align=\"center\"/>\n",
        "  <p style=\"text-align:center; font-style:italic;\">\n",
        "    Uma malha de pontos uniformemente espaçados.\n",
        "  </p> \n",
        "  </center>\n",
        "</div>\n",
        "\n",
        "Para que seja possível tratar numericamente as equações diferenciais, elas devem ser expressas sob a forma de operações aritméticas que o computador possa executar. Essen- cialmente, devemos representar os operadores diferenciais (contínuos) presentes na ED por expressões algébricas (discretas), ou seja, devemos discretizar a ED. Portanto, antes de resolver a ED numericamente é preciso encontrar, para os termos que nela aparecem, as respectivas expressões escritas em função dos pontos (finitos) da malha. Essas expressões são denominadas aproximações por diferenças finitas. O resultado final desse processo é uma equação algébrica denominada equação de diferenças finitas (EDF). A EDF é escrita para cada ponto da região discretizada em que se deseja calcular a solução do problema. Resolvendo-se as EDFs, encontra-se a solução aproximada do problema. Tal solução não é exata devido a:\n",
        "\n",
        "- erros inerentes ao processo de discretização das equação;\n",
        "- erros de arredondamento nos cálculos feitos pelo computador;\n",
        "- erros na aproximação numérica das condições auxiliares.  \n",
        "\n",
        "Pode-se obter uma aproximação de diferenças finitas diretamente da definição de derivada de uma função $f$ contínua,\n",
        "\n",
        "\\begin{equation*}\n",
        "\\frac{df}{dx} = \\lim_{\\Delta x\\to 0}\\frac{f(x+\\Delta x)-f(x)}{\\Delta x}.\n",
        "\\end{equation*}\n",
        "\n",
        "Para tanto, basta que $\\Delta x$ assuma um valor fixo (não-nulo) — que chamaremos de $h$ — ao invés de tender a zero, para que o lado direito da equação represente uma aproximação (avançada) de diferenças finitas:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\frac{df}{dx} \\approx \\frac{f(x+h)-f(x)}{h}.\n",
        "\\end{equation*}\n",
        "\n",
        "Desse modo, utilizando-se dois valores de $f$ separados por uma distância (finita) $\\Delta x$, a expressão representa uma aproximação algébrica para a primeira derivada de $f$. Essa situação está ilustrada na figura abaixo, onde os dois pontos, $x$ e $x+\\Delta x$, afastados entre si por uma distância $\\Delta x$, formam a reta secante cuja declividade serve de aproximação para a derivada da função $f$ no ponto $x$. Quando a separação $\\Delta x$ diminui, a reta secante se aproxima da reta tangente (derivada real), melhorando assim o valor estimado para a derivada.\n",
        "\n",
        "\n",
        "<div style=\"margin:auto; text-align:center; width:75%;\">\n",
        "  <center>\n",
        "  <img src=\"https://raw.githubusercontent.com/bgeneto/MCA/main/imagens/derivada.png\" align=\"center\"/>\n",
        "  <p style=\"text-align:center; font-style:italic;\">\n",
        "    Aproximação da derivada primeira da função genérica f(x) no ponto x por uma reta secante.\n",
        "  </p> \n",
        "  </center>\n",
        "</div>\n",
        "\n",
        "Aproximações de diferenças finitas, como a mostrada anteriormente, podem ser obtidas de várias formas. As técnicas mais comuns são a expansão em série de Taylor e a interpolação polinomial. O método da expansão em série de Taylor será utilizado na obtenção de aproximações de diferenças finitas de primeira e segunda ordem para as derivadas primeira, segunda e mista de uma função $f$. A técnica de interpolação é geralmente utilizada no contexto em que se faz necessário o uso de malhas cujo espaçamento não é uniforme.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B1vgyzMLVUUk"
      },
      "source": [
        "## Aproximações para a Derivada Primeira\n",
        "---\n",
        "\n",
        "As aproximações de diferenças finitas têm como base a expansão em série de Taylor de uma função $f$. Supondo que $f$ seja contínua no intervalo $[a,b]$ de interesse e que possua derivadas até ordem $N$ contínuas nesse intervalo, o teorema de Taylor nos permite escrever, para todo ponto $x \\in [a,b]$,\n",
        "\n",
        "\\begin{equation*}\n",
        "f(x) = f(x_0) + (\\Delta x)\\left. \\frac{df}{dx} \\right|_{x_0} + \\frac{{(\\Delta x)^2 }}\n",
        "{{2!}}\\left. \\frac{d^2 f}{dx^2} \\right|_{x_0} + \\frac{{(\\Delta x)^3 }}\n",
        "{{3!}}\\left. \\frac{d^3 f}{dx^3} \\right|_{x_0 } +  \\cdots  + R_N,\n",
        "\\end{equation*}\n",
        "\n",
        "em que $\\Delta x = x - x_0$ e $R_N$ é o resto (de Lagrange), definido como\n",
        "\n",
        "\\begin{equation*}\n",
        "R_N = \\frac{{(\\Delta x)^N}}{{N!}}\\left.\\frac{d^Nf}{dx^N}\\right|_\\xi,\\quad\\xi\\in [a,b].\n",
        "\\end{equation*}\n",
        "\n",
        "Para aproximar a derivada primeira de uma função $f$ no ponto $x_i$ vamos expandir $f(x_i+\\Delta x)$ em série de Taylor em torno do ponto $x_i$,\n",
        "\n",
        "\\begin{equation*}\n",
        "f(x_i  + \\Delta x) = f(x_i) + (\\Delta x)\\left. {\\frac{df}{dx}} \\right|_{x_i}  + \\frac{{(\\Delta x)^2 }}\n",
        "{{2!}}\\left. \\frac{d^2 f}{dx^2} \\right|_{x_i}  + \\frac{{(\\Delta x)^3 }}\n",
        "{{3!}}\\left. {\\frac{d^3 f}{dx^3}} \\right|_{x_i}  +  \\cdots\\,,\n",
        "\\end{equation*}\n",
        "\n",
        "onde as reticências indicam os termos restantes da série de Taylor até o resto $R_N$. Após isolar a primeira derivada, podemos escrever\n",
        "\n",
        "\\begin{equation*}\n",
        "\\left. {\\frac{df}{dx}} \\right|_{x_i}  = \\frac{{f(x_i  + \\Delta x) - f(x_i )}}\n",
        "{{\\Delta x}} + \\left[ { - \\frac{{(\\Delta x)}}\n",
        "{{2!}}\\left. {\\frac{d^2 f}{dx^2}} \\right|_{x_i}  - \\frac{{(\\Delta x)^2 }}\n",
        "{{3!}}\\left. {\\frac{d^3 f}{dx^3}} \\right|_{x_i}  -  \\cdots } \\right].\n",
        "\\end{equation*}\n",
        "\n",
        "Note que, para isolar a primeira derivada, todos os termos da série de Taylor foram divididos pelo espaçamento $\\Delta x$. Podemos então dizer que a primeira derivada é igual ao quociente\n",
        "\n",
        "\\begin{equation*}\n",
        "\\frac{{f(x_i + \\Delta x) - f(x_i )}}{{\\Delta x}},\n",
        "\\end{equation*}\n",
        "\n",
        "mais o erro local de truncamento (ELT), dado por:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\left[ { - \\frac{{(\\Delta x)}}\n",
        "{{2!}}\\left. {\\frac{d^2 f}{dx^2}} \\right|_{x_i}  - \\frac{{(\\Delta x)^2 }}\n",
        "{{3!}}\\left. {\\frac{d^3 f}{dx^3}} \\right|_{x_i}  -  \\cdots } \\right].\n",
        "\\end{equation*}\n",
        "\n",
        "O \\ELT aparece naturalmente devido à utilização de um número finito de termos na série de Taylor. Como não podemos tratar os infinitos termos dessa série na aproximação numérica para a derivada de $f$, a série foi truncada a partir da derivada de segunda ordem inclusive. O \\ELT fornece uma medida da diferença entre o valor exato da derivada e sua aproximação numérica, indicando também que essa diferença varia linearmente com a redução do espaçamento $\\Delta x$, isto é, com o refinamento da malha. Assim, para reduzirmos o erro por quatro, por exemplo, devemos utilizar um espaçamento $\\frac{1}{4}$ do original e portanto, quatro vezes mais pontos na malha. Dessa forma, os termos do \\ELT serão representados por $O(\\Delta x)$. Deve-se notar que uma expressão do tipo $O(\\Delta x)$ só indica como \\ELT varia com o refinamento da malha, e não o valor do erro.\n",
        "\n",
        "Podemos simplificar a notação se escrevendo $f_i$ para $f(x_i)$ ou, em geral, $f_{i\\pm k}$ para $f(x_i\\pm k\\Delta x)$. Com isso, a equação anterior torna-se:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\left. {\\frac{df}{dx}} \\right|_{x_i}  = \\frac{{f_{i + 1}  - f_i }}{{\\Delta x}} + O(\\Delta x),\n",
        "\\end{equation*}\n",
        "\n",
        "que é uma equação de diferenças finitas que representa uma aproximação de primeira ordem para a primeira derivada de $f$, utilizando diferença avançada, visto que no cálculo da derivada no ponto $x_i$ foi utilizado um ponto adiante de $x_i$, no caso, $x_{i+1}$. A declividade (primeira derivada) de $f$ em $x_i$ é aproximada pela declividade da reta secante formada pelos pontos $(x_i,f_i)$ e $(x_{i+1},f_{i+1})$, conforme mostra a \\Figref[vref]{avancada}.\n",
        "\n",
        "<div style=\"margin:auto; text-align:center; width:75%;\">\n",
        "  <center>\n",
        "  <img src=\"https://raw.githubusercontent.com/bgeneto/MCA/main/imagens/avancada.png\" align=\"center\"/>\n",
        "  <p style=\"text-align:center; font-style:italic;\">\n",
        "    Pontos utilizados na aproximação para a primeira derivada de f(x) por diferença avançada.\n",
        "  </p> \n",
        "  </center>\n",
        "</div>\n",
        "\n",
        "Uma segunda aproximação de diferenças finitas pode ser obtida a partir da expansão de $f(x_i-\\Delta x)$ em série de Taylor em torno do ponto $x_i$:\n",
        "\n",
        "\\begin{equation*}\n",
        "f(x_i  - \\Delta x) = f(x_i ) - (\\Delta x)\\left. {\\frac{df}{dx}} \\right|_{x_i}  + \\frac{{(\\Delta x)^2 }}\n",
        "{{2!}}\\left. {\\frac{d^2f}{dx^2}} \\right|_{x_i}  + O(\\Delta x)^3.\n",
        "\\end{equation*}\n",
        "\n",
        "Isolando a primeira derivada, temos\n",
        "\n",
        "\\begin{equation*}\n",
        "\\left. {\\frac{df}{dx}} \\right|_{x_i}  = \\frac{{f_i  - f_{i - 1} }}{{\\Delta x}} + O(\\Delta x),\n",
        "\\end{equation*}\n",
        "\n",
        "que é outra aproximação de primeira ordem para a primeira derivada de $f$. Diferentemente da fórmula avançada, na qual utiliza-se um ponto adiante de $x_i$, a fórmula atrasada utiliza o ponto $x_{i-1}$, ponto este anterior a $x_i$. Por essa razão, essa equação de diferenças é chamada de aproximação por diferenças atrasadas. A \\Figref[vref]{atrasada} mostra os pontos utilizados nessa aproximação. A declividade da função~$f$ no ponto $x_i$ é aproximada pela declividade da reta secante aos pontos $(x_{i-1},f_{i-1})$ e $(x_{i},f_{i})$.\n",
        "\n",
        "\n",
        "<div style=\"margin:auto; text-align:center; width:75%;\">\n",
        "  <center>\n",
        "  <img src=\"https://raw.githubusercontent.com/bgeneto/MCA/main/imagens/atrasada.png\" align=\"center\"/>\n",
        "  <p style=\"text-align:center; font-style:italic;\">\n",
        "    Pontos utilizados na aproximação para a primeira derivada de f(x) por diferença atrasada.\n",
        "  </p> \n",
        "  </center>\n",
        "</div>\n",
        "\n",
        "\n",
        "Até agora obtivemos somente aproximações de primeira ordem para a derivada primeira de $f$, uma aproximação de $O(\\Delta x)^2$ ainda para a primeira derivada pode ser obtida ao subtrairmos as expansões em série de Taylor,
        "\\begin{equation*}\n",
        "f(x_i  + \\Delta x) - f(x_i  - \\Delta x) = 2(\\Delta x)\\left. {\\frac{df}{dx}} \\right|_{x_i}  + O(\\Delta x)^3,\n",
        "\\end{equation*}\n",
        "\n",
        "ou, isolando a derivada,\n",
        "\n",
        "\\begin{equation*}\n",
        "\\left.{\\frac{df}{dx}}\\right|_{x_i} = \\frac{{f_{i+1} - f_{i-1}}}{{2\\Delta x}} + O(\\Delta x)^2.\n",
        "\\end{equation*}\n",
        "\n",
        "Note que a aproximação central utiliza os pontos $x_{i-1}$ e $x_{i+1}$ para o cálculo da primeira derivada de $f$ no ponto central $x_i$. Por essa razão, ela é denominada aproximação por diferenças centrais. Neste caso, conforme mostra a \\Figref[vref]{central}, a derivada de $f$ em $x_i$ é aproximada pela declividade da reta secante que passa pelos pontos $(x_{i-1},f_{i-1})$ e $(x_{i+1},f_{i+1})$.\n",
        "\n",
        "\n",
        "<div style=\"margin:auto; text-align:center; width:75%;\">\n",
        "  <center>\n",
        "  <img src=\"https://raw.githubusercontent.com/bgeneto/MCA/main/imagens/central.png\" align=\"center\"/>\n",
        "  <p style=\"text-align:center; font-style:italic;\">\n",
        "    Pontos utilizados na aproximação de segunda ordem para a primeira derivada de f(x) por diferença central.\n",
        "  </p> \n",
        "  </center>\n",
        "</div>\n",
        "\n",
        "No caso de aproximações de segunda ordem, reduções sucessivas no passo $\\Delta x$ da malha provocam uma redução quadrática no erro da aproximação da primeira derivada de $f$ pela aproximação central. Ao dividirmos o passo por dois, por exemplo, o erro é dividido por quatro, sem precisarmos de quatro vezes mais pontos, como nas expressões de primeira ordem. Isso é uma propriedade extremamente útil, já que, com menor número de pontos e, portanto, menor esforço computacional, podemos conseguir uma aproximação melhor que aquelas fornecidas pelas aproximações atrasada e avançada.\n",
        "\n",
        "Em resumo, as três fórmulas para a primeira derivada de $f$ deduzidas anteriormente, a partir da expansão em série de Taylor, são:\n",
        "\n",
        "\\begin{align*}\n",
        "\\left.\\frac{df}{dx}\\right|_{x_i} &\\approx \\frac{f_{i+1} - f_i}{\\Delta x}, && \\text{(fórmula avançada)}\\\\[12pt]\n",
        "\\left.\\frac{df}{dx}\\right|_{x_i} &\\approx \\frac{f_{i} - f_{i-1}}{\\Delta x}, && \\text{(fórmula atrasada)}\\\\\n",
        "\\left.\\frac{df}{dx}\\right|_{x_i} &\\approx \\frac{f_{i+1} - f_{i-1}}{2\\Delta x}. && \\text{(fórmula central)}\n",
        "\\end{align*}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tarefa:** Tome a série de Taylor de $f$ em torno de $a = x_i$ e calcule a série em $x = x_{i−2}$, $x_{i−1}$, $x_{i+1}$, $x_{i+2}$. Mostre que as equações resultantes podem ser combinadas para formar uma aproximação para $f'(x_i)$ de ordem $\\mathcal O(h^4)$. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# f(x_{i−2}) − 8f(x_{i−1}) + 8f(x_{i−1}) − f(x_{i+2}). "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "c6f5971c4b8f55c07f21d358e9303b9e57f31eec2f7748b3e83dfc05726d64d8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
